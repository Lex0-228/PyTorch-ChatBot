# Цей файл визначає базові функції обробки природної мови, які використовуються для обробки вхідних даних.

import numpy as np
import nltk
from uk_stemmer import UkStemmer
stemmer = UkStemmer()

def tokenize(sentence):
    """
    розбиває вхідний текст на токени (≈слова)
    """
    return nltk.word_tokenize(sentence)


def stem(word):
    """
    повертає стем слова (у цьому випадку = частину без флексії). На жаль, бібліотека NLTK не має вбудованого стеммера
    для укр. мови, та і загалом поки не існує дійсно якісної програми для виконання цього завдання
    (як мінімум мене не дуже влаштовують результати цього стеммера, але кращого варіянта не було)
    """
    return stemmer.stem_word(word.lower())


def bag_of_words(tokenized_sentence, words):
    """
    принцип "мішка слів" - повертає вектор, у якому загальна кількість координат = загальна кількість слів,
    для кожного конкретного випадку певна координата набуває значення 1, якщо це слово наявне в ньому (інакше - 0)
    """
    # пропускаємо слова через стеммер
    sentence_words = [stem(word) for word in tokenized_sentence]
    # створюємо вектор-шаблон з нулів
    bag = np.zeros(len(words), dtype=np.float32)
    # нумеруємо слова та задаємо значення відповідного індекса в "мішку" рівне 1, якщо відповідне слово трапляється
    for idx, w in enumerate(words):
        if w in sentence_words: 
            bag[idx] = 1

    return bag